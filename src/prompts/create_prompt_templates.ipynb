{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6cabea-9571-4070-800a-d55a07294a6b",
   "metadata": {},
   "source": [
    "This notebook saves prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631000b1-a192-4dd6-8dc7-a478c1eeed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac271f14-d28c-45f2-baa8-b69113538f90",
   "metadata": {},
   "source": [
    "# web_nlg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effd3246-0cfc-466d-b3cb-1a9a220353a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"web_nlg\", \"release_v3.0_en\", \"train\", cache_dir = \"../../cache\")\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "icl_per_prompt = 1\n",
    "\n",
    "instructions = [\n",
    "    \"Following is a set of knowledge graph triples. Generate a coherent piece of text that contains all of the information in the triples. Only use information from the provided triples.\",\n",
    "    \"Generate a coherent paragraph incorporating the information from the given set of knowledge graph triples.\",\n",
    "    \"Formulate a cohesive passage using the details provided in the set of knowledge graph triples.\",\n",
    "]\n",
    "\n",
    "prompt_fpath = Path(\"assets/web_nlg_1_shot.json\") \n",
    "prompts = []\n",
    "for i in range(3):\n",
    "    icl_df = train_df.sample(icl_per_prompt, random_state=i)\n",
    "    prompt = instructions[i] + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "\n",
    "        text = row[\"lex\"][\"text\"][0]\n",
    "        \n",
    "        prompt += \"### Triples\\n\"\n",
    "        for triple in row[\"modified_triple_sets\"]['mtriple_set'][0]:\n",
    "            prompt += f\"{triple}\\n\"\n",
    "        prompt += \"\\n### Text\\n\" + text + \"\\n\"\n",
    "        prompt += \"\\n### Triples\\n{triples}\\n\\n### Text\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806090d-954e-4e5f-926e-704a64f8cea5",
   "metadata": {},
   "source": [
    "# e2e_nlg\n",
    "\n",
    "## 5 ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3980c8a-3158-4ef8-b5fe-5961e4aad597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3611"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTION = \"Transform the meaning representation into a sentence.\"\n",
    "dataset = datasets.load_dataset(\"e2e_nlg\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)\n",
    "\n",
    "prompt_fpath = Path(\"assets/e2e_nlg_1_shot.json\") \n",
    "prompts = []\n",
    "for i in range(5):\n",
    "    icl_df = dataset.sample(icl_per_prompt, random_state=i)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Meaning representation: {meaning_representation}\\nNatural language: {human_reference}\\n\\n\".format(**row)\n",
    "    prompt += \"Meaning representation: {meaning_representation}\\nNatural language:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c37182-f71f-443e-9b98-a858f748bdcb",
   "metadata": {},
   "source": [
    "## 7 ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27004464-91b7-4e97-86cd-ee64abe2776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4926"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTION = \"Transform the meaning representation into a sentence.\"\n",
    "dataset = datasets.load_dataset(\"e2e_nlg\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)\n",
    "\n",
    "prompt_fpath = Path(\"assets/e2e_nlg_1_shot_7_prompts.json\") \n",
    "prompts = []\n",
    "for i in range(7):\n",
    "    icl_df = dataset.sample(icl_per_prompt, random_state=i)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Meaning representation: {meaning_representation}\\nNatural language: {human_reference}\\n\\n\".format(**row)\n",
    "    prompt += \"Meaning representation: {meaning_representation}\\nNatural language:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aba6fc-4e6b-4272-8c9e-6562c8af0dbd",
   "metadata": {},
   "source": [
    "## 3 ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03a1c1be-fee1-4a38-a135-2f71528c647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meaning_representation</th>\n",
       "      <th>human_reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name[The Vaults], eatType[pub], priceRange[mor...</td>\n",
       "      <td>The Vaults pub near Café Adriatic has a 5 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name[The Cambridge Blue], eatType[pub], food[E...</td>\n",
       "      <td>Close to Café Brazil, The Cambridge Blue pub s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name[The Eagle], eatType[coffee shop], food[Ja...</td>\n",
       "      <td>The Eagle is a low rated coffee shop near Burg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              meaning_representation  \\\n",
       "0  name[The Vaults], eatType[pub], priceRange[mor...   \n",
       "1  name[The Cambridge Blue], eatType[pub], food[E...   \n",
       "2  name[The Eagle], eatType[coffee shop], food[Ja...   \n",
       "\n",
       "                                     human_reference  \n",
       "0  The Vaults pub near Café Adriatic has a 5 star...  \n",
       "1  Close to Café Brazil, The Cambridge Blue pub s...  \n",
       "2  The Eagle is a low rated coffee shop near Burg...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTION = \"Transform the meaning representation into a sentence.\"\n",
    "dataset = datasets.load_dataset(\"e2e_nlg\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d0de377-37b7-40b7-a50d-76f2fdf9cc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2151"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/e2e_nlg_1_shot_3_prompts.json\") \n",
    "prompts = []\n",
    "for _ in range(3):\n",
    "    icl_df = dataset.sample(icl_per_prompt)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Meaning representation: {meaning_representation}\\nNatural language: {human_reference}\\n\\n\".format(**row)\n",
    "    prompt += \"Meaning representation: {meaning_representation}\\nNatural language:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57413ec3-05e7-4c3e-a57e-4adac9b90cb9",
   "metadata": {},
   "source": [
    "10 ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52c5b3e-550d-4f73-901a-0974a98e4779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6986"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTION = \"Transform the meaning representation into a sentence.\"\n",
    "dataset = datasets.load_dataset(\"e2e_nlg\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)\n",
    "\n",
    "prompt_fpath = Path(\"assets/e2e_nlg_1_shot_10_prompts.json\") \n",
    "prompts = []\n",
    "for i in range(10):\n",
    "    icl_df = dataset.sample(icl_per_prompt, random_state=i)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Meaning representation: {meaning_representation}\\nNatural language: {human_reference}\\n\\n\".format(**row)\n",
    "    prompt += \"Meaning representation: {meaning_representation}\\nNatural language:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c4437-82cf-42e7-ba6f-bd04225e8a51",
   "metadata": {},
   "source": [
    "# cnn/dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aadf489-6d5c-43db-a0b2-333d497ff799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n",
       "2  06352019a19ae31e527f37f7571c6dd7f0c5da37  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTIONS = [\n",
    "    \"Write a short summary of the article.\",\n",
    "    \"What are the highlights of the article?\",\n",
    "    \"Produce a few-sentence description of the article.\"\n",
    "]\n",
    "\n",
    "dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55113b2f-b7e6-423f-a759-b076684e9f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/cnn_dailymail_0_shot.json\") \n",
    "prompts = []\n",
    "for instruction in INSTRUCTIONS:\n",
    "    icl_df = dataset.sample(icl_per_prompt)\n",
    "    prompt = \"{article}\" + f\"\\n\\n{instruction}\\nSummary:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9051c024-c5b9-467b-a545-687ee58f2f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 6) (4097407462.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"What are the main points of the above article?\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 6)\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTIONS = [\n",
    "    \"Write a short summary of the article.\",\n",
    "    \"What are the highlights of the article?\",\n",
    "    \"Produce a few-sentence description of the article.\",\n",
    "    \"Summarize the main takeaways from the article.\",\n",
    "    \"What are the main points of the above article?\n",
    "]\n",
    "\n",
    "dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)\n",
    "\n",
    "prompt_fpath = Path(\"assets/cnn_dailymail_0_shot_5_prompts.json\") \n",
    "prompts = []\n",
    "for instruction in INSTRUCTIONS:\n",
    "    prompt = \"{article}\" + f\"\\n\\n{instruction}\\nSummary:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb8655-dfbf-4b64-9bc3-17636de2bcac",
   "metadata": {},
   "source": [
    "# xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f490616-3de5-4ccb-a23d-d99acf8ad4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The full cost of damage in Newton Stewart, one...</td>\n",
       "      <td>Clean-up operations are continuing across the ...</td>\n",
       "      <td>35232142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fire alarm went off at the Holiday Inn in Ho...</td>\n",
       "      <td>Two tourist buses have been destroyed by fire ...</td>\n",
       "      <td>40143035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferrari appeared in a position to challenge un...</td>\n",
       "      <td>Lewis Hamilton stormed to pole position at the...</td>\n",
       "      <td>35951548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  The full cost of damage in Newton Stewart, one...   \n",
       "1  A fire alarm went off at the Holiday Inn in Ho...   \n",
       "2  Ferrari appeared in a position to challenge un...   \n",
       "\n",
       "                                             summary        id  \n",
       "0  Clean-up operations are continuing across the ...  35232142  \n",
       "1  Two tourist buses have been destroyed by fire ...  40143035  \n",
       "2  Lewis Hamilton stormed to pole position at the...  35951548  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INSTRUCTION = \"Summarize the article.\"\n",
    "icl_per_prompt = 1\n",
    "dataset = datasets.load_dataset(\"EdinburghNLP/xsum\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3054e6-5451-43a7-a9f3-088cbfe1da00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7906"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/xsum_1_shot.json\") \n",
    "prompts = []\n",
    "for _ in range(3):\n",
    "    icl_df = dataset.sample(icl_per_prompt)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Article: {document}\\nSummary: {summary}\\n\\n\".format(**row)\n",
    "    prompt += \"Article: {document}\\nSummary:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55abb390-d9f8-444e-86f2-a8986029a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████| 304M/304M [00:01<00:00, 202MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████| 16.7M/16.7M [00:00<00:00, 45.0MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████| 17.0M/17.0M [00:00<00:00, 33.7MB/s]\n",
      "Generating train split: 100%|████████████████████████████████████████████████████████████████| 204045/204045 [00:01<00:00, 191960.47 examples/s]\n",
      "Generating validation split: 100%|█████████████████████████████████████████████████████████████| 11332/11332 [00:00<00:00, 207794.40 examples/s]\n",
      "Generating test split: 100%|███████████████████████████████████████████████████████████████████| 11334/11334 [00:00<00:00, 192706.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15593"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INSTRUCTION = \"Summarize the article.\"\n",
    "icl_per_prompt = 1\n",
    "dataset = datasets.load_dataset(\"EdinburghNLP/xsum\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "\n",
    "prompt_fpath = Path(\"assets/xsum_1_shot_5_prompts.json\") \n",
    "prompts = []\n",
    "for _ in range(5):\n",
    "    icl_df = dataset.sample(icl_per_prompt)\n",
    "    prompt = INSTRUCTION + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Article: {document}\\nSummary: {summary}\\n\\n\".format(**row)\n",
    "    prompt += \"Article: {document}\\nSummary:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e38213-8ba5-475a-9608-9850d1aed05a",
   "metadata": {},
   "source": [
    "# gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792d4bc8-c743-4be0-9341-0e43cd5c53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cots = [\n",
    "        \"\"\"Question: Ivan has a bird feeder in his yard that holds two cups of birdseed. Every week, he has to refill the emptied feeder. Each cup of birdseed can feed fourteen birds, but Ivan is constantly chasing away a hungry squirrel that steals half a cup of birdseed from the feeder every week. How many birds does Ivan’s bird feeder feed weekly?\n",
    "Let's think step by step\n",
    "The squirrel steals 1/2 cup of birdseed every week, so the birds eat 2 - 1/2 = 1 1/2 cups of birdseed.\n",
    "Each cup feeds 14 birds, so Ivan’s bird feeder feeds 14 * 1 1/2 = 21 birds weekly.\n",
    "The answer is 21\"\"\",\n",
    "        \"\"\"Question: Samuel took 30 minutes to finish his homework while Sarah took 1.3 hours to finish it. How many minutes faster did Samuel finish his homework than Sarah?\n",
    "Let's think step by step\n",
    "Since there are 60 minutes in 1 hour, then 1.3 hours is equal to 1.3 x 60 = 78 minutes.\n",
    "Thus, Samuel is 78 – 30 = 48 minutes faster than Sarah.\n",
    "The answer is 48\"\"\",\n",
    "        \"\"\"Question: Julia bought 3 packs of red balls, 10 packs of yellow balls, and 8 packs of green balls. There were 19 balls in each package. How many balls did Julie buy in all?\n",
    "Let's think step by step\n",
    "The total number of packages is 3 + 10 + 8 = 21.\n",
    "Julia bought 21 × 19 = 399 balls.\n",
    "The answer is 399\"\"\",\n",
    "        \"\"\"Question: Lexi wants to run a total of three and one-fourth miles. One lap on a particular outdoor track measures a quarter of a mile around. How many complete laps must she run?\n",
    "Let's think step by step\n",
    "There are 3/ 1/4 = 12 one-fourth miles in 3 miles.\n",
    "So, Lexi will have to run 12 (from 3 miles) + 1 (from 1/4 mile) = 13 complete laps.\n",
    "The answer is 13\"\"\",\n",
    "        \"\"\"Question: Asia bought a homecoming dress on sale for $140. It was originally priced at $350. What percentage off did she get at the sale?\n",
    "Let's think step by step\n",
    "Asia saved $350 - $140 = $210 on the dress.\n",
    "That means she saved $210 / $350 = 0.60 or 60% off on the dress.\n",
    "The answer is 60\"\"\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cadfc7c1-e4fb-4b4d-87ad-a36521b1ad61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "for cot in cots:\n",
    "    prompt = f\"Answer the math problem.\\n\\n{cot}\\n\\n\"\n",
    "    prompt += \"Questions: {question}\\nLet's think step by step\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath = Path(\"assets/gsm8k_1_shot_cot_simple.json\") \n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830edc12",
   "metadata": {},
   "source": [
    "# common gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd3f482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_set_idx</th>\n",
       "      <th>concepts</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[ski, mountain, skier]</td>\n",
       "      <td>Skier skis down the mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[ski, mountain, skier]</td>\n",
       "      <td>A skier is skiing down a mountain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[ski, mountain, skier]</td>\n",
       "      <td>Three skiers are skiing on a snowy mountain.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   concept_set_idx                concepts  \\\n",
       "0                0  [ski, mountain, skier]   \n",
       "1                0  [ski, mountain, skier]   \n",
       "2                0  [ski, mountain, skier]   \n",
       "\n",
       "                                         target  \n",
       "0                  Skier skis down the mountain  \n",
       "1            A skier is skiing down a mountain.  \n",
       "2  Three skiers are skiing on a snowy mountain.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"allenai/common_gen\", split=\"train\", cache_dir = \"../../cache\").to_pandas()\n",
    "icl_per_prompt = 2\n",
    "\n",
    "instructions = [\n",
    "        \"Following is a set of words. Generate a coherent sentence that contains all of the words.\",\n",
    "        \"Generate a coherent sentence incorporating the words from the given set.\",\n",
    "        \"Formulate a cohesive sentence using the words provided.\",\n",
    "        \"Create a unified sentence that encompasses all the words.\",\n",
    "        \"Compose a seamless sentence integrating the words given.\",\n",
    "]\n",
    "\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99203e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1766"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/common_gen_1_shot_2_prompts.json\") \n",
    "prompts = []\n",
    "for i in range(5):\n",
    "    icl_df = dataset.sample(icl_per_prompt, random_state=i)\n",
    "    prompt = instructions[i] + \"\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"\\n### Words\\n\"\n",
    "        prompt += str(row[\"concepts\"]) + \"\\n\"\n",
    "        prompt += \"\\n### Sentence\\n\" + row[\"target\"] + \"\\n\"\n",
    "    prompt += \"\\n### Words\\n{concepts}\\n\\n### Sentence\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a3333-7ab4-410f-995f-a1dbee04a7c9",
   "metadata": {},
   "source": [
    "## Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9830599f-8031-480a-b54d-eed277983882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/squad.json\") \n",
    "prompts = [\"{context}\\n\\nQuestion: {question}\\nAnswer:\"]\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533adf82-160a-4c07-942c-03353e794189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "      <td>...</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "      <td>...</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be4db0acb8001400a502ef</td>\n",
       "      <td>...</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc_id  \\\n",
       "0  56be4db0acb8001400a502ec   \n",
       "1  56be4db0acb8001400a502ee   \n",
       "2  56be4db0acb8001400a502ef   \n",
       "\n",
       "                                                text                    value  \\\n",
       "0                                                ...           Denver Broncos   \n",
       "1                                                ...  Santa Clara, California   \n",
       "2                                                ...           Denver Broncos   \n",
       "\n",
       "           title                                            context  \\\n",
       "0  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
       "1  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
       "2  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                            question  \n",
       "0  Which NFL team represented the AFC at Super Bo...  \n",
       "1                Where did Super Bowl 50 take place?  \n",
       "2                  Which NFL team won Super Bowl 50?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icl_per_prompt = 2\n",
    "INSTRUCTIONS = [\n",
    "    \"Answer the question given the context. Be as brief as possible.\",\n",
    "    \"Respond to the inquiry considering the surrounding circumstances. Make sure your answer is correct.\",\n",
    "    \"You are an LLM who is supposed to be as factual as possible. Address the question in light of the given context.\",\n",
    "]\n",
    "    \n",
    "\n",
    "dataset = datasets.load_dataset(\"hazyresearch/based-squad\", split=\"validation\", cache_dir = \"../../cache\").to_pandas()\n",
    "dataset = dataset.iloc[:len(dataset) // 2] # Train is first half\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92cd8515-c3a1-4683-9044-75034e885359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1065"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/squad_1_shot.json\") \n",
    "prompts = [\n",
    "    \"\"\"Answer the question given the context. Be as brief as possible.\n",
    "\n",
    "Context: While experimenting, Tesla inadvertently faulted a power station generator, causing a power outage. In August 1917, Tesla explained what had happened in The Electrical Experimenter: \"As an example of what has been done with several hundred kilowatts of high frequency energy liberated, it was found that the dynamos in a power house six miles away were repeatedly burned out, due to the powerful high frequency currents set up in them, and which caused heavy sparks to jump through the windings and destroy the insulation!\"\n",
    "Question: What did Tesla accidentally cause?\n",
    "Answer: power outage\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\", \n",
    "\n",
    "    \"\"\"Respond to the inquiry considering the surrounding circumstances. Make sure your answer is correct.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\",\n",
    "\n",
    "    \"\"\"You are an LLM who is supposed to be as factual as possible. Address the question in light of the given context.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "]\n",
    "for idx in range(3):\n",
    "    continue\n",
    "    icl_df = dataset.sample(icl_per_prompt, random_state = idx)\n",
    "    prompt = INSTRUCTIONS[idx] + \"\\n\\n\"\n",
    "    for row in icl_df.to_dict(orient=\"records\"):\n",
    "        prompt += \"Context: {context}\\nQuestion: {question}\\nAnswer: {value}\\n\\n\".format(**row)\n",
    "    prompt += \"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    print(prompt)\n",
    "    print(\"-\"*30)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2a67-f1b2-4a7e-9524-31082c1e4075",
   "metadata": {},
   "source": [
    "## Definition Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525797a1-63e1-4402-a3d7-b457e393121c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_fpath = Path(\"assets/definition_extraction_0_shot.json\") \n",
    "prompts = [\n",
    "    \"For the sentence below, identify the term that is being defined.\\n\\nSentence: {text}\\nTerm:\",\n",
    "    \"{text}\\n\\nQuestion: What are the term(s) being defined in the sentence above?\\nAnswer:\",\n",
    "    \"{text}\\n\\nWhat word or phrase is being defined in the sentence above?\"\n",
    "]\n",
    "prompt_fpath.write_text(json.dumps(prompts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be52386-af1d-433c-a78d-c58c7a35babf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
